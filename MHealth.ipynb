{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paevjc/Human-Activity-Recognition/blob/main/MHealth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHPKYcfsHajg"
      },
      "source": [
        "# DSA Internal Project: Group 4\n",
        "### Notebook Author: Jeriel, Elly, Shaniah, Harshni, Ziyan\n",
        "### Dataset: MHealth Dataset\n",
        "### Source: Kaggle https://www.kaggle.com/datasets/nirmalsankalana/mhealth-dataset-data-set-csv/data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPlupEfPofLA"
      },
      "source": [
        "Problem Statement: Individuals in healthcare settings, such as physiotherapists or elderly care providers, would benefit from an automated system that accurately classifies human activities using wearable sensor data. The system should provide real-time insights into activities, aiding in the monitoring of patient mobility and overall health.\n",
        "The goal is to create a machine learning pipeline that can process accelerometer and gyroscope data and classify activities accurately, enabling healthcare providers to make informed decisions about patient care.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxfQZgXRpzuM"
      },
      "source": [
        "# Data Summary (In words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUJFnmikMNdd"
      },
      "source": [
        "\n",
        "#### Activities: 12\n",
        "L1: Standing still (1 min)\\\n",
        "L2: Sitting and relaxing (1 min)\\\n",
        "L3: Lying down (1 min)\\\n",
        "L4: Walking (1 min)\\\n",
        "L5: Climbing stairs (1 min)\\\n",
        "L6: Waist bends forward (20x)\\\n",
        "L7: Frontal elevation of arms (20x)\\\n",
        "L8: Knees bending (crouching) (20x)\\\n",
        "L9: Cycling (1 min)\\\n",
        "L10: Jogging (1 min)\\\n",
        "L11: Running (1 min)\\\n",
        "L12: Jump front & back (20x)\n",
        "\n",
        "#### Sensor Devices: 2\n",
        "alx: acceleration from the left-ankle sensor (X axis)\\\n",
        "aly: acceleration from the left-ankle sensor (Y axis)\\\n",
        "alz: acceleration from the left-ankle sensor (Z axis)\\\n",
        "glx: gyro from the left-ankle sensor (X axis)\\\n",
        "gly: gyro from the left-ankle sensor (Y axis)\\\n",
        "glz: gyro from the left-ankle sensor (Z axis)\\\n",
        "arx: acceleration from the right-lower-arm sensor (X axis)\\\n",
        "ary: acceleration from the right-lower-arm sensor (Y axis)\\\n",
        "arz: acceleration from the right-lower-arm sensor (Z axis)\\\n",
        "grx: gyro from the right-lower-arm sensor (X axis)\\\n",
        "gry: gyro from the right-lower-arm sensor (Y axis)\\\n",
        "grz: gyro from the right-lower-arm sensor (Z axis)\\\n",
        "subject: volunteer number\\\n",
        "Activity: corresponding activity\n",
        "\n",
        "#### Subjects: 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DOun4Dkzn9O"
      },
      "source": [
        "# Upload Necessary Files\n",
        "\n",
        "*   Mhealth zip\n",
        "*   requirement.txt\n",
        "*   Mhealth.db\n",
        "*   template.env\n",
        "*   Set GPU: T4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDPUMHuBFhB2"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import uuid\n",
        "import hashlib\n",
        "import zipfile\n",
        "import sqlite3\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "from typing import List, Sequence, TypedDict\n",
        "from typing_extensions import Annotated\n",
        "from PIL import Image  # Image is imported from PIL\n",
        "\n",
        "# Data Processing and Analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "# Machine Learning Libraries (Deep Learning)\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, SeparableConv1D, MaxPooling1D, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Plotting Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Table\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "# Others\n",
        "from tqdm import tqdm\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akDTD9qf1Xgn"
      },
      "source": [
        "# 2. Loading of Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From Zip File"
      ],
      "metadata": {
        "id": "S1Y8HXAZ5pIW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ki4R6im9HfY5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define file paths\n",
        "zip_file_path = 'Mhealth.zip'\n",
        "extraction_path = 'Mhealth/'\n",
        "\n",
        "# Extract files from the ZIP archive\n",
        "with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extraction_path)\n",
        "\n",
        "# Initialize an empty list to store the DataFrames\n",
        "dfs = []\n",
        "\n",
        "# Loop through each CSV file in the extracted folder\n",
        "for root, dirs, files in os.walk(extraction_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.csv'):\n",
        "            file_path = os.path.join(root, file)\n",
        "            print(f\"Reading {file_path}\")\n",
        "            # Read each CSV file into a DataFrame\n",
        "            df = pd.read_csv(file_path)\n",
        "            dfs.append(df)\n",
        "\n",
        "# Combine all DataFrames into a single DataFrame\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Display the combined DataFrame (first few rows)\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "combined_df.to_csv('mhealth_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yWoAJAIWQa-"
      },
      "source": [
        "# 3. SQL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting to SQL Database\n",
        "conn = sqlite3.connect(\"mhealth.db\")\n",
        "\n",
        "# Saving DataFrame to SQLite database\n",
        "df.to_sql(\"mobile_health\", conn, if_exists=\"replace\", index=False)"
      ],
      "metadata": {
        "id": "alQo4YzO0x6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving DataFrame to SQLite database\n",
        "mhealth_data = \"\"\"\n",
        "SELECT *\n",
        "FROM mobile_health;\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_sql_query(mhealth_data, conn)\n",
        "display(df)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "nDbkVyW06An_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_act = \"\"\"\n",
        "SELECT DISTINCT Activity\n",
        "FROM mobile_health;\n",
        "\"\"\"\n",
        "\n",
        "df_act = pd.read_sql_query(query_act, conn)\n",
        "print(\"Distinct Activity Values with Data Types:\")\n",
        "print(df_act.dtypes)  # Displays data types\n",
        "print(df_act.to_string(index=False))"
      ],
      "metadata": {
        "id": "U0nNycOLmRsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmh6Xbta5XP3"
      },
      "source": [
        "### 3.1 Querying the *database*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IQGgkY-GFjE"
      },
      "outputs": [],
      "source": [
        "# Activity Duration Analysis\n",
        "\n",
        "## Purpose:\n",
        "# 1. Measures the time (in seconds) each subject spends on different activities, where data is sampled at 50Hz (every 0.02 seconds).\n",
        "# 2.Counts the total samples for each activity.\n",
        "\n",
        "duration_query = \"\"\"\n",
        "SELECT\n",
        "    subject,\n",
        "    Activity,\n",
        "    COUNT(*) * 0.02 as duration_seconds, -- 50Hz sampling rate\n",
        "    COUNT(*) as sample_count\n",
        "FROM mobile_health\n",
        "GROUP BY subject, Activity\n",
        "ORDER BY subject, duration_seconds DESC;\n",
        "\"\"\"\n",
        "\n",
        "duration_df = pd.read_sql_query(duration_query, conn)\n",
        "display(duration_df.head())  # Display the first few rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX9pTQLm5p4s"
      },
      "source": [
        "From the result: Identifies dominant activities for each subject, helping understand behavioral patterns.\n",
        "Useful in activity recognition or health monitoring systems to track activity balance, e.g., time spent sitting vs. walking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g7W2Kj759yG"
      },
      "outputs": [],
      "source": [
        "# Sensor Pattern Analysis\n",
        "\n",
        "# Purpose:\n",
        "# 1. Computes average and standard deviation of accelerometer readings for each activity.\n",
        "# 2. Helps summarize sensor data patterns for different activities.\n",
        "\n",
        "sensor_pattern_query = \"\"\"\n",
        "SELECT\n",
        "    Activity,\n",
        "    AVG(alx) as avg_left_accel_x,\n",
        "    AVG(aly) as avg_left_accel_y,\n",
        "    AVG(alz) as avg_left_accel_z,\n",
        "    SQRT(AVG(alx * alx) - AVG(alx) * AVG(alx)) as std_left_accel_x,\n",
        "    SQRT(AVG(aly * aly) - AVG(aly) * AVG(aly)) as std_left_accel_y,\n",
        "    SQRT(AVG(alz * alz) - AVG(alz) * AVG(alz)) as std_left_accel_z\n",
        "FROM mobile_health\n",
        "GROUP BY Activity;\n",
        "\"\"\"\n",
        "\n",
        "sensor_pattern_df = pd.read_sql_query(sensor_pattern_query, conn)\n",
        "display(sensor_pattern_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QT5FdQS6Qd3"
      },
      "source": [
        "With result: \\\n",
        "Machine Learning Models: These statistics can be used as features in models to classify activities.\n",
        "Activity Characterization: Helps differentiate activities based on unique motion patterns (e.g., walking vs. running)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxqsT6yi6WYz"
      },
      "outputs": [],
      "source": [
        "# Peak Acceleration Analysis\n",
        "\n",
        "# Purpose:\n",
        "# Identifies the top 5 highest acceleration readings for each activity.\n",
        "\n",
        "peak_accel_query = \"\"\"\n",
        "WITH ranked_accel AS (\n",
        "    SELECT\n",
        "        *,\n",
        "        ROW_NUMBER() OVER (PARTITION BY Activity ORDER BY SQRT(alx*alx + aly*aly + alz*alz) DESC) as rn\n",
        "    FROM mobile_health\n",
        ")\n",
        "SELECT * FROM ranked_accel\n",
        "WHERE rn <= 5\n",
        "ORDER BY Activity, rn;\n",
        "\"\"\"\n",
        "\n",
        "peak_accel_df = pd.read_sql_query(peak_accel_query, conn)\n",
        "display(peak_accel_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eihNUPIv6jD0"
      },
      "source": [
        "Anomaly Detection: High accelerations might indicate sudden movements (e.g., falls, jumps).\\\n",
        "Feature Engineering: Peak values can act as distinguishing features in activity recognition systems.\\\n",
        "Biomechanics Analysis: Useful in sports or rehabilitation settings to study motion extremes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3yjAG8g63Wt"
      },
      "source": [
        "Behavioral Insights: Reveals common transitions (e.g., sitting to walking), providing a better understanding of activity sequences.\\\n",
        "Healthcare Applications: Useful for monitoring recovery progress, e.g., increasing transitions from resting to walking post-surgery.\\\n",
        "Workflow Optimization: Can be applied in workplace studies to improve ergonomic efficiency by reducing unnecessary transitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5oCQzqm6tJi"
      },
      "outputs": [],
      "source": [
        "# Activity Transitions\n",
        "\n",
        "# Purpose:\n",
        "# Counts how often subjects transition from one activity to another.\n",
        "\n",
        "activity_transitions_query = \"\"\"\n",
        "WITH activity_changes AS (\n",
        "    SELECT\n",
        "        *,\n",
        "        LAG(Activity) OVER (PARTITION BY subject ORDER BY ROWID) as prev_activity\n",
        "    FROM mobile_health\n",
        ")\n",
        "SELECT\n",
        "    prev_activity,\n",
        "    Activity as current_activity,\n",
        "    COUNT(*) as transition_count\n",
        "FROM activity_changes\n",
        "WHERE prev_activity != Activity\n",
        "GROUP BY prev_activity, Activity\n",
        "ORDER BY transition_count DESC;\n",
        "\"\"\"\n",
        "\n",
        "activity_transitions_df = pd.read_sql_query(activity_transitions_query, conn)\n",
        "display(activity_transitions_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpZa5m8pIJrm"
      },
      "source": [
        "# 4. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36extnjx7VIW"
      },
      "source": [
        "### 4.1 Cleaning of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t42q9iejIJXP"
      },
      "outputs": [],
      "source": [
        "# summary of dataframe\n",
        "display(df)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUDT2NkJIa3J"
      },
      "outputs": [],
      "source": [
        "#check for null values\n",
        "print(df.isnull().sum().to_dict()) # no null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDwrbWiTIkB1"
      },
      "outputs": [],
      "source": [
        "# check for number of observations to see is resampling us necessary\n",
        "df.subject.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCL2Xo3sJRVQ"
      },
      "outputs": [],
      "source": [
        "df['Activity'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJtkzud-JZSH"
      },
      "outputs": [],
      "source": [
        "# check if data is imbalanced\n",
        "df['Activity'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8gVxWthoJyDZ"
      },
      "outputs": [],
      "source": [
        "# bar plot for visualisation of imbalanced activity ditribution\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.countplot(x='Activity', data = df, order = df['Activity'].value_counts().index)\n",
        "plt.title('Number of samples by Activity')\n",
        "plt.xlabel('Activity')\n",
        "plt.ylabel('Count of Samples')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9_pjKV7Nh8u"
      },
      "source": [
        "As seen in the bar plot, there is significant imbalance with the majority of the samples having class label 'Activity 0'. While 'Activity 12' have the least representation in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "749skaHBUeoS"
      },
      "outputs": [],
      "source": [
        "# resampling(downsampling) activity 0 to 30720 observations\n",
        "\n",
        "df_activity_0 = df[df['Activity'] == 0]\n",
        "df_activity_else = df[df['Activity'] != 0]\n",
        "\n",
        "df_activity_0 = df_activity_0.sample(n=30720, random_state=1)\n",
        "df = pd.concat([df_activity_0, df_activity_else])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LOZmtXjUv44"
      },
      "outputs": [],
      "source": [
        "df['Activity'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb1pHGgHTTsE"
      },
      "outputs": [],
      "source": [
        "#check for null values\n",
        "print(df.isnull().sum().to_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fwoe35BXTZ2D"
      },
      "outputs": [],
      "source": [
        "# drop duplicates\n",
        "df = df.drop_duplicates(keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V61tIl4DJcY3"
      },
      "outputs": [],
      "source": [
        "# visualisation of how signal values of x,y,z dimensions cary with time\n",
        "\n",
        "# map activity labels\n",
        "activity_map = {\n",
        "    0: 'Doing nothing',\n",
        "    1: 'Standing still (1 min)',\n",
        "    2: 'Sitting and relaxing (1 min)',\n",
        "    3: 'Lying down (1 min)',\n",
        "    4: 'Walking (1 min)',\n",
        "    5: 'Climbing stairs (1 min)',\n",
        "    6: 'Waist bends forward (20x)',\n",
        "    7: 'Frontal elevation of arms (20x)',\n",
        "    8: 'Knees bending (crouching) (20x)',\n",
        "    9: 'Cycling (1 min)',\n",
        "    10: 'Jogging (1 min)',\n",
        "    11: 'Running (1 min)',\n",
        "    12: 'Jump front & back (20x)'\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGY-lT0IW8Gg"
      },
      "outputs": [],
      "source": [
        "sensors = {\n",
        "    'Left Ankle - Accelerometer': ['alx', 'aly', 'alz'],\n",
        "    'Left Ankle - Gyroscope': ['glx', 'gly', 'glz'],\n",
        "    'Right Lower Arm - Accelerometer': ['arx', 'ary', 'arz'],\n",
        "    'Right Lower Arm - Gyroscope': ['grx', 'gry', 'grz']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXISTonDGDz0",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "print(\"Activity Map:\")\n",
        "pprint(activity_map)\n",
        "\n",
        "print(\"\\nSensors:\")\n",
        "pprint(sensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAuaZcxy7uQV"
      },
      "source": [
        "### 4.2 Visualisations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXsuH6fz-3Sg",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# bar plot for visualisation of corrected activity ditribution\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.countplot(x='Activity', data = df, order = df['Activity'].value_counts().index)\n",
        "plt.title('Number of Readings by Activity')\n",
        "plt.xlabel('Activity')\n",
        "plt.ylabel('Count of Readings')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQpk7xlsaQHC",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (30,10))\n",
        "sns.countplot(x = 'subject',  hue = 'Activity', data = df)\n",
        "plt.title('Activity Readings by Subjects')\n",
        "plt.xlabel('Activities by Subject')\n",
        "plt.ylabel('Count of Readings')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())  # Check the data\n",
        "print(f\"Unique Activities: {df['Activity'].unique()}\")  # Verify activity labels\n"
      ],
      "metadata": {
        "id": "XwOcIA9QeyqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter data where Activity = 0\n",
        "activity_0_df = df[df['Activity'] == 0]\n",
        "\n",
        "# Count the number of readings for each sensor per subject\n",
        "sensor_counts = activity_0_df.groupby('subject')[['alx', 'aly', 'alz', 'glx', 'gly', 'glz',\n",
        "                                                  'arx', 'ary', 'arz', 'grx', 'gry', 'grz']].count()\n",
        "\n",
        "# Print the result\n",
        "print(\"Sensor Reading Counts for Activity 0 per Subject:\\n\")\n",
        "print(sensor_counts.to_string())\n"
      ],
      "metadata": {
        "id": "XCHW30KucrU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaQlKw34RibC"
      },
      "outputs": [],
      "source": [
        "print(\"Data Preview:\")\n",
        "from IPython.display import display  # Enables rich output for tables in Colab\n",
        "display(df.head())  # This will show the data in a table format\n",
        "\n",
        "# Display unique activities in table format\n",
        "unique_activities = pd.DataFrame(df['Activity'].unique(), columns=['Unique Activities'])\n",
        "print(\"Unique Activities:\")\n",
        "display(unique_activities)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfvdKs4FXhVU"
      },
      "source": [
        "Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_v802azYI98"
      },
      "outputs": [],
      "source": [
        "def plot_comparison_part1(df):\n",
        "    \"\"\"Plots the second half of activities (0 to 5).\"\"\"\n",
        "    # Define sensor groups for visualization\n",
        "    for i in range(0, 6):  # Loop through activities 0 to 5\n",
        "        # Filter data for the current activity\n",
        "        activity_data = df[df['Activity'] == i]\n",
        "\n",
        "        # Check if there is no data for the current activity\n",
        "        if activity_data.empty:\n",
        "            print(f\"No data available for Activity {i}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Create a subplot for the current activity\n",
        "        com_fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=[\n",
        "                f'{activity_map.get(i, f\"Activity {i}\")} - {sensor_name}' for sensor_name in sensors.keys()\n",
        "            ],\n",
        "            shared_yaxes=True\n",
        "        )\n",
        "\n",
        "        for idx, (sensor_name, columns) in enumerate(sensors.items()):\n",
        "            row = (idx // 2) + 1\n",
        "            col = (idx % 2) + 1\n",
        "\n",
        "            for column in columns:\n",
        "                com_fig.add_trace(go.Scatter(\n",
        "                    x=activity_data.reset_index(drop=True).index,\n",
        "                    y=activity_data.reset_index(drop=True)[column],\n",
        "                    mode='lines',\n",
        "                    name=column,\n",
        "                    line=dict(width=1.5)\n",
        "                ), row=row, col=col)\n",
        "\n",
        "        # Update layout for each subplot\n",
        "        com_fig.update_layout(\n",
        "            title=f'Activity: {activity_map.get(i, f\"Activity {i}\")}',\n",
        "            title_x=0.5,\n",
        "            height=800,\n",
        "            xaxis_title='Time Index',\n",
        "            yaxis_title='Sensor Value',\n",
        "            showlegend=True\n",
        "        )\n",
        "\n",
        "        # Show the plot\n",
        "        com_fig.show()\n",
        "\n",
        "plot_comparison_part1(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_F51s1mgYJA2"
      },
      "outputs": [],
      "source": [
        "def plot_comparison_part2(df):\n",
        "    \"\"\"Plots the second half of activities (7 to 12).\"\"\"\n",
        "    for i in range(6, 13):  # Loop through activities 7 to 12\n",
        "        # Filter data for the current activity\n",
        "        activity_data = df[df['Activity'] == i]\n",
        "\n",
        "        # Check if there is no data for the current activity\n",
        "        if activity_data.empty:\n",
        "            print(f\"No data available for Activity {i}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Create a subplot for the current activity\n",
        "        com_fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=[\n",
        "                f'{activity_map.get(i, f\"Activity {i}\")} - {sensor_name}' for sensor_name in sensors.keys()\n",
        "            ],\n",
        "            shared_yaxes=True\n",
        "        )\n",
        "\n",
        "        for idx, (sensor_name, columns) in enumerate(sensors.items()):\n",
        "            row = (idx // 2) + 1\n",
        "            col = (idx % 2) + 1\n",
        "\n",
        "            for column in columns:\n",
        "                com_fig.add_trace(go.Scatter(\n",
        "                    x=activity_data.reset_index(drop=True).index,\n",
        "                    y=activity_data.reset_index(drop=True)[column],\n",
        "                    mode='lines',\n",
        "                    name=column,\n",
        "                    line=dict(width=1.5)\n",
        "                ), row=row, col=col)\n",
        "\n",
        "        # Update layout for each subplot\n",
        "        com_fig.update_layout(\n",
        "            title=f'Activity: {activity_map.get(i, f\"Activity {i}\")}',\n",
        "            title_x=0.5,\n",
        "            height=800,\n",
        "            xaxis_title='Time Index',\n",
        "            yaxis_title='Sensor Value',\n",
        "            showlegend=True\n",
        "        )\n",
        "\n",
        "        # Show the plot\n",
        "        com_fig.show()\n",
        "\n",
        "plot_comparison_part2(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSEQCBc-u0z_"
      },
      "outputs": [],
      "source": [
        "# Plot category : Creates horizontal bar plots to visualize the percentage distribution of a categorical variable.\n",
        "# Purpose\n",
        "# Summarize Categorical Data:\n",
        "# Provides a quick overview of the distribution of categories.\n",
        "# Example: If cat is Activity, it shows the percentage distribution of activities.\n",
        "# Spot Imbalances:\n",
        "# Highlights whether certain categories dominate the dataset, which is crucial for model building and balancing.\n",
        "\n",
        "# Purpose: To create horizontal bar plots that show the percentage distribution of a categorical variable (e.g., 'Activity')\n",
        "\n",
        "def plot_category(df, cat):\n",
        "    # Calculate the percentage distribution\n",
        "    array = (df[cat].value_counts().sort_values(ascending=False) / len(df)) * 100\n",
        "\n",
        "    # Convert the percentage values to string format with a '%' symbol\n",
        "    # text_values = array.values.round(1).astype(str) + '%'  # Original line causing error\n",
        "    # The correct way to add '%' is to use list comprehension and format strings:\n",
        "    text_values = [f'{x:.1f}%' for x in array.values]\n",
        "\n",
        "    # Create the bar chart using Plotly Express\n",
        "    cat_fig = px.bar(\n",
        "        x=array.values,  # Percentage values\n",
        "        y=array.index,   # Category names\n",
        "        labels={'x': 'Percentage', 'y': cat},  # Axis labels\n",
        "        title=f'{cat} Distribution',  # Plot title\n",
        "        text=text_values,  # Add percentage text on bars\n",
        "        orientation='h',  # Horizontal bar chart\n",
        "        color=array.index,  # Color bars by category\n",
        "        color_discrete_sequence=px.colors.qualitative.Set3  # Set color palette\n",
        "    )\n",
        "\n",
        "    # Customize layout\n",
        "    cat_fig.update_layout(\n",
        "        title_x=0.5,  # Center the title\n",
        "        height=500,  # Set figure height\n",
        "        showlegend=False,  # Hide the legend\n",
        "        xaxis_title='Percentage',  # Label for x-axis\n",
        "        yaxis_title=cat,  # Label for y-axis\n",
        "    )\n",
        "\n",
        "    # Display the chart\n",
        "    cat_fig.show()\n",
        "\n",
        "plot_category(df, 'Activity')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIpQvd9qKjuc",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Create correlation matrix excluding Activity and subject\n",
        "corr_matrix = df.iloc[:, 0:-2].corr()\n",
        "\n",
        "# Convert the correlation matrix to a format Plotly can use\n",
        "corr_matrix = corr_matrix.reset_index().melt(id_vars=['index'])\n",
        "\n",
        "# Create the heatmap with Plotly\n",
        "cm_fig = px.imshow(corr_matrix.pivot(index='index', columns='variable', values='value'),\n",
        "                color_continuous_scale='RdBu_r',\n",
        "                labels={'index': 'Features', 'variable': 'Features', 'value': 'Correlation'},\n",
        "                title=\"Correlation Heatmap\")\n",
        "\n",
        "cm_fig.update_layout(\n",
        "    xaxis_title='Features',\n",
        "    yaxis_title='Features',\n",
        "    title_x=0.5,  # Center the title\n",
        ")\n",
        "\n",
        "cm_fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4lRX2SyfVWp"
      },
      "outputs": [],
      "source": [
        "# PCA\n",
        "#used here to reduce the dimensionality of sensor data (accelerometer and gyroscope measurements),making it easier to visualize and analyze activity patterns while preserving as much variance as possible.\n",
        "#This allows for more efficient classification and feature extraction from the high-dimensional data.\n",
        "\n",
        "X = df.drop(columns=['Activity', 'subject'])  # Dropping Activity and subject columns for PCA\n",
        "\n",
        "# Perform PCA (reduce to 2 components)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
        "\n",
        "# Add the 'Activity' column back to the PCA dataframe\n",
        "pca_df['Activity'] = df['Activity']\n",
        "\n",
        "# Plotly Express scatter plot for PCA\n",
        "pca_fig = px.scatter(pca_df, x='PC1', y='PC2', color='Activity',\n",
        "                 title='PCA Scatter Plot',\n",
        "                 labels={'PC1': 'Principal Component 1', 'PC2': 'Principal Component 2'},\n",
        "                 color_continuous_scale='viridis')\n",
        "\n",
        "# Show the plot\n",
        "pca_fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NjkQ07vQeKj-"
      },
      "outputs": [],
      "source": [
        "# Accelerometer for Ankle\n",
        "# Plot for each axis (e.g., 'alx', 'aly', 'alz')\n",
        "\n",
        "fig1 = px.box(df, x='Activity', y='alx', color='Activity',\n",
        "              title=\"Left Ankle Accelerometer X-axis Readings Across Activities\",\n",
        "              labels={'Activity': 'Activity', 'alx': 'Acceleration X-axis'})\n",
        "\n",
        "fig2 = px.box(df, x='Activity', y='aly', color='Activity',\n",
        "              title=\"Left Ankle Accelerometer Y-axis Readings Across Activities\",\n",
        "              labels={'Activity': 'Activity', 'aly': 'Acceleration Y-axis'})\n",
        "\n",
        "fig3 = px.box(df, x='Activity', y='alz', color='Activity',\n",
        "              title=\"Left Ankle Accelerometer Z-axis Readings Across Activities\",\n",
        "              labels={'Activity': 'Activity', 'alz': 'Acceleration Z-axis'})\n",
        "\n",
        "# Show all the plots\n",
        "fig1.show()\n",
        "fig2.show()\n",
        "fig3.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oBhUUCABHckX"
      },
      "outputs": [],
      "source": [
        "# Gyroscope for Ankle\n",
        "# Plot for each axis (e.g., 'glx', 'gly', 'glz')\n",
        "\n",
        "fig4 = px.box(df, x='Activity', y='glx', color='Activity',\n",
        "              title=\"Left Ankle Gyroscope X-axis Readings Across Activities\",\n",
        "              labels={'Activity': 'Activity', 'glx': 'Rotation X-axis'})\n",
        "\n",
        "fig5 = px.box(df, x='Activity', y='gly', color='Activity',\n",
        "              title=\"Left Ankle Gyroscope Y-axis Readings Across Activities\",\n",
        "              labels={'Activity': 'Activity', 'gly': 'Rotation Y-axis'})\n",
        "\n",
        "fig6 = px.box(df, x='Activity', y='glz', color='Activity',\n",
        "              title=\"Left Ankle Gyroscope Z-axis Readings Across Activities\",\n",
        "              labels={'Activity': 'Activity', 'glz': 'Rotation Z-axis'})\n",
        "\n",
        "# Show all the plots\n",
        "fig4.show()\n",
        "fig5.show()\n",
        "fig6.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "etV70NaxHdQ2"
      },
      "outputs": [],
      "source": [
        "# Accelerometer for Arm\n",
        "# Plot for each axis (e.g., 'arx', 'ary', 'arz')\n",
        "\n",
        "fig7 = px.box(df, x='Activity', y='arx', color='Activity',\n",
        "              title=\"Right Lower Arm Accelerometer X-axis Readings Across Activities\",\n",
        "              labels={'Activity': 'Activity', 'arx': 'Acceleration X-axis'})\n",
        "\n",
        "fig8 = px.box(df, x='Activity', y='ary', color='Activity',\n",
        "              title=\"Right Lower Arm Accelerometer Y-axis Readings Across Activities\",\n",
        "              labels={'Activity': 'Activity', 'ary': 'Acceleration Y-axis'})\n",
        "\n",
        "fig9 = px.box(df, x='Activity', y='arz', color='Activity',\n",
        "              title=\"Right Lower Arm Accelerometer Z-axis Readings Across Activities\",\n",
        "              labels={'Activity': 'Activity', 'arz': 'Acceleration Z-axis'})\n",
        "\n",
        "# Show all the plots\n",
        "fig7.show()\n",
        "fig8.show()\n",
        "fig9.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIHgFdn9Ax3m"
      },
      "outputs": [],
      "source": [
        "# Gyroscope for Arm\n",
        "# Plot for each axis (e.g., 'grx', 'gry', 'grz')\n",
        "\n",
        "fig10 = px.box(df, x='Activity', y='grx', color='Activity',\n",
        "              title=\"Right Lower Arm Gyroscope X-axis Readings Across Activities\",\n",
        "              labels={'Activity': 'Activity', 'grx': 'Rotation X-axis'})\n",
        "\n",
        "fig11 = px.box(df, x='Activity', y='gry', color='Activity',\n",
        "              title=\"Right Lower Arm Gyroscope Y-axis Readings Across Activities\",\n",
        "              labels={'Activity': 'Activity', 'gry': 'Rotation Y-axis'})\n",
        "\n",
        "fig12 = px.box(df, x='Activity', y='grz', color='Activity',\n",
        "              title=\"Right Lower Arm Gyroscope Z-axis Readings Across Activities\",\n",
        "              labels={'Activity': 'Activity', 'grz': 'Rotation Z-axis'})\n",
        "\n",
        "# Show all the plots\n",
        "fig10.show()\n",
        "fig11.show()\n",
        "fig12.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g94NAqsOj4C"
      },
      "source": [
        "# 5. Data Preprocessing and Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIX4w5stTwZO"
      },
      "source": [
        "# Standardisation and Splitting of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZExR198IvbLg"
      },
      "outputs": [],
      "source": [
        "# Make a copy of the dataframe before modifying\n",
        "X = df.drop(['Activity','subject'], axis=1).copy()\n",
        "y = df['Activity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NdJjJnDvf2a"
      },
      "outputs": [],
      "source": [
        "# Train-test splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cTWonkYvRdd"
      },
      "outputs": [],
      "source": [
        "# Segment features into numerical and categorical\n",
        "num_features = ['alx', 'aly', 'alz', 'glx', 'gly', 'glz', 'arx', 'ary', 'arz', 'grx', 'gry', 'grz']\n",
        "# cat_features = ['subject']\n",
        "\n",
        "# Numerical transformer (Impute missing values and scale numerical features)\n",
        "num_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean\n",
        "    ('scaler', StandardScaler())  # Scale the features\n",
        "])\n",
        "\n",
        "# Categorical transformer (Impute missing values and apply one-hot encoding)\n",
        "# cat_transformer = Pipeline(steps=[\n",
        "#     ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with most frequent category\n",
        "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode the categorical feature\n",
        "# ])\n",
        "\n",
        "# Apply both transformers\n",
        "data_transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('numerical', num_transformer, num_features)\n",
        "        # ('categorical', cat_transformer, cat_features)\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjDgbK_mvi5g"
      },
      "outputs": [],
      "source": [
        "# Label encoding for target variable\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Apply label encoding separately to the target variable (as it's not part of the pipeline)\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkm2oDNKgqmY"
      },
      "source": [
        "### 5.1 Pipeline and Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjZ1IyTavlIk"
      },
      "outputs": [],
      "source": [
        "# Function to create model pipeline with preprocessing and classifier\n",
        "def create_pipeline(model):\n",
        "    return Pipeline(steps=[\n",
        "        ('data_transformer', data_transformer),  # Apply preprocessing steps\n",
        "        ('classifier', model)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWjXIYfSpY90"
      },
      "outputs": [],
      "source": [
        "# # Function to plot confusion matrix\n",
        "def plot_confusion_matrix(y_test_encoded, y_pred, model_name, activity_map):\n",
        "    cm = confusion_matrix(y_test_encoded, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=activity_map.values(),\n",
        "                yticklabels=activity_map.values())\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.xlabel('Predicted Activity')\n",
        "    plt.ylabel('Actual Activity')\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot confusion matrix using Plotly Express\n",
        "'''def plot_confusion_matrix(y_test_encoded, y_pred, model_name, activity_map):\n",
        "    cm = confusion_matrix(y_test_encoded, y_pred)\n",
        "\n",
        "    # Convert confusion matrix to a DataFrame for better plotting with Plotly\n",
        "    cm_df = pd.DataFrame(cm, index=activity_map.values(), columns=activity_map.values())\n",
        "\n",
        "    # Plot the confusion matrix using Plotly Express\n",
        "    fig = px.imshow(cm_df,\n",
        "                    labels={'x': 'Predicted Activity', 'y': 'Actual Activity'},\n",
        "                    title=f'Confusion Matrix - {model_name}',\n",
        "                    color_continuous_scale='Blues',\n",
        "                    text_auto=True)\n",
        "    fig.update_xaxes(side=\"top\")  # To put the predicted labels at the top\n",
        "    fig.show()'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezPOpoTpYbmT"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_model(model, X_train, X_test, y_train_encoded, y_test_encoded, model_name, activity_map):\n",
        "    # Pipelining\n",
        "    pipeline = create_pipeline(model)\n",
        "    pipeline.fit(X_train, y_train_encoded)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    print(f\"{model_name} Accuracy: {accuracy_score(y_test_encoded, y_pred)}\")\n",
        "    print(f\"\\n{model_name} Classification Report:\")\n",
        "    print(classification_report(y_test_encoded, y_pred))\n",
        "\n",
        "    plot_confusion_matrix(y_test_encoded, y_pred, model_name, activity_map)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST CAN DELETE AFTER\n",
        "\n",
        "def train_and_evaluate_model(model, X_train, X_test, y_train_encoded, y_test_encoded, model_name, activity_map):\n",
        "    # Pipelining\n",
        "    pipeline = create_pipeline(model)\n",
        "    pipeline.fit(X_train, y_train_encoded)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Check if the model is a regressor or classifier\n",
        "    if isinstance(model, (LinearRegression,)):  # Add other regressors if needed\n",
        "        # Use regression metrics\n",
        "        from sklearn.metrics import mean_squared_error, r2_score\n",
        "        print(f\"{model_name} Mean Squared Error: {mean_squared_error(y_test_encoded, y_pred)}\")\n",
        "        print(f\"{model_name} R-squared: {r2_score(y_test_encoded, y_pred)}\")\n",
        "    else:\n",
        "        # Use classification metrics\n",
        "        print(f\"{model_name} Accuracy: {accuracy_score(y_test_encoded, y_pred)}\")\n",
        "        print(f\"\\n{model_name} Classification Report:\")\n",
        "        print(classification_report(y_test_encoded, y_pred))\n",
        "        plot_confusion_matrix(y_test_encoded, y_pred, model_name, activity_map)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "UvYOxJj6hLP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRWUFMz3wF9f"
      },
      "outputs": [],
      "source": [
        "def tune_hyperparameters(model, param_grid, X_train, y_train, X_test, y_test, scoring='accuracy', cv=5):\n",
        "    pipeline = create_pipeline(model)\n",
        "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=scoring, cv=cv, n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "    cv_score = grid_search.best_score_\n",
        "    test_score = accuracy_score(y_test, best_model.predict(X_test))\n",
        "\n",
        "    results_df = pd.DataFrame({\n",
        "        \"Best Model\": [best_model],\n",
        "        \"Best Parameters\": [best_params],\n",
        "        \"CV Score\": [cv_score],\n",
        "        \"Test Score\": [test_score]\n",
        "    })\n",
        "\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBzZolMEgkfX"
      },
      "source": [
        "Since we have cleaned our MHealth data (by dopping irrelevant columns), encoded our vairables, train-test split as well as standardised our data, we proceed to classification models. We will be using Logistic Regression, K-Nearest Neighbours, Support Vector Machines and Random Forest Trees. After which we will compare the accuracy scores from all the models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1ulofOP5qGa"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aK0XurASgO2i"
      },
      "outputs": [],
      "source": [
        "model_lr = joblib.load('LogReg.pkl')\n",
        "print(\"LogReg loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zKjF6pmTgR4R"
      },
      "outputs": [],
      "source": [
        "model_knn = joblib.load('KNN.pkl')\n",
        "print(\"KNN loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf = RandomForestClassifier(random_state=42)\n",
        "train_and_evaluate_model(model_rf, X_train, X_test, y_train_encoded, y_test_encoded, \"Random Forest\", activity_map)"
      ],
      "metadata": {
        "id": "ERyRjWnOM40f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(model_rf, 'RF.pkl')\n",
        "print(\"Model saved as 'RF.pkl'\")"
      ],
      "metadata": {
        "id": "LSnwNkj2Nhi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cnIvbsFEgR-w"
      },
      "outputs": [],
      "source": [
        "model_rf = joblib.load('RF.pkl')\n",
        "print(type(model_rf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IYwFngP9gSGb"
      },
      "outputs": [],
      "source": [
        "model_svm = joblib.load('SVM.pkl')\n",
        "print(\"SVM loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_DT = joblib.load('DT.pkl')\n",
        "print(\"DT loaded successfully!\")"
      ],
      "metadata": {
        "id": "ZknW4Aklme59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JatVZKczgqnH"
      },
      "outputs": [],
      "source": [
        "model_linr = joblib.load('LinR.pkl')\n",
        "print(\"LinR loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Reports\n",
        "\n",
        "If you want to view the the model accuracy and classification reports."
      ],
      "metadata": {
        "id": "oeosKz2eoCgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_transformed = data_transformer.transform(X_test)\n",
        "y_pred = model_lr.predict(X_test_transformed)\n",
        "\n",
        "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
        "print(f\"Logistic Regression Accuracy: {accuracy:.10f}\\n\")\n",
        "\n",
        "report = classification_report(y_test_encoded, y_pred, target_names=activity_map.values())\n",
        "print(\"Logistic Regression Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "TGx0Y6ePoUOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_transformed = data_transformer.transform(X_test)\n",
        "y_pred = model_knn.predict(X_test_transformed)\n",
        "\n",
        "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
        "print(f\"KNN Accuracy: {accuracy:.10f}\\n\")\n",
        "\n",
        "report = classification_report(y_test_encoded, y_pred, target_names=activity_map.values())\n",
        "print(\"KNN Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "9scBf7V8oqTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_transformed = data_transformer.transform(X_test)\n",
        "y_pred = model_svm.predict(X_test_transformed)\n",
        "\n",
        "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
        "print(f\"SVM Accuracy: {accuracy:.10f}\\n\")\n",
        "\n",
        "report = classification_report(y_test_encoded, y_pred, target_names=activity_map.values())\n",
        "print(\"SVM Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "2EqKHYjDni4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_transformed = data_transformer.transform(X_test)\n",
        "y_pred = model_rf.predict(X_test_transformed)\n",
        "\n",
        "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
        "print(f\"Random Forest Accuracy: {accuracy:.10f}\\n\")\n",
        "\n",
        "report = classification_report(y_test_encoded, y_pred, target_names=activity_map.values())\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "6AXt3_BYoqr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_transformed = data_transformer.transform(X_test)\n",
        "y_pred = model_DT.predict(X_test_transformed)\n",
        "\n",
        "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
        "print(f\"Decision Tree Accuracy: {accuracy:.10f}\\n\")\n",
        "\n",
        "report = classification_report(y_test_encoded, y_pred, target_names=activity_map.values())\n",
        "print(\"Decision Tree Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "BkzfTMFPoqbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJleyG0uf410"
      },
      "source": [
        "# Model Training (ONLY RUN IF NOT USING PKL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uXTBX1T14XuS"
      },
      "outputs": [],
      "source": [
        "model_lr = LogisticRegression(random_state=42, max_iter=1000)\n",
        "train_and_evaluate_model(model_lr, X_train, X_test, y_train_encoded, y_test_encoded, \"Logistic Regression\", activity_map)\n",
        "\n",
        "# Save the model using joblib\n",
        "joblib.dump(model_lr, 'LogReg.pkl')\n",
        "print(\"Model saved as 'LogReg.pkl'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SU0ppWP84bxp"
      },
      "outputs": [],
      "source": [
        "model_knn = KNeighborsClassifier()\n",
        "train_and_evaluate_model(model_knn, X_train, X_test, y_train_encoded, y_test_encoded, \"KNN\", activity_map)\n",
        "\n",
        "# Save the model using joblib\n",
        "joblib.dump(model_knn, 'KNN.pkl')\n",
        "print(\"Model saved as 'KNN.pkl'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "03iGGB5bpgsw"
      },
      "outputs": [],
      "source": [
        "model_rf = RandomForestClassifier(random_state=42)\n",
        "train_and_evaluate_model(model_rf, X_train, X_test, y_train_encoded, y_test_encoded, \"Random Forest\", activity_map)\n",
        "\n",
        "#Save the model using joblib\n",
        "joblib.dump(model_rf, 'RF.pkl')\n",
        "print(\"Model saved as 'RF.pkl'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pNZGbn3nZhB2"
      },
      "outputs": [],
      "source": [
        "model_svm = SVC(kernel='rbf', random_state=1, gamma=0.1)\n",
        "train_and_evaluate_model(model_svm, X_train, X_test, y_train_encoded, y_test_encoded, \"SVM\", activity_map)\n",
        "\n",
        "#Save the model using joblib\n",
        "joblib.dump(model_svm, 'SVM.pkl')\n",
        "print(\"Model saved as 'SVM.pkl'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yhIl9o8TlyKQ"
      },
      "outputs": [],
      "source": [
        "model_dt = DecisionTreeClassifier(random_state=42)\n",
        "train_and_evaluate_model(model_dt, X_train, X_test, y_train_encoded, y_test_encoded, \"Decision Tree\", activity_map)\n",
        "\n",
        "# Save the model using joblib\n",
        "joblib.dump(model_dt, 'DT.pkl')\n",
        "print(\"Model saved as 'DT.pkl'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6pKWwsJ84HPZ"
      },
      "outputs": [],
      "source": [
        "model_linr = LinearRegression()\n",
        "train_and_evaluate_model(model_linr, X_train, X_test, y_train_encoded, y_test_encoded, \"Linear Regression\", activity_map)\n",
        "joblib.dump(model_linr, 'LinR.pkl')\n",
        "print(\"Model saved as 'LinReg.keras'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbcokze35t2q"
      },
      "source": [
        "# Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkoAJvrBJkNK"
      },
      "outputs": [],
      "source": [
        "param_grid_lr = {\n",
        "    'C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
        "    'solver': ['liblinear', 'lbfgs'],  # Optimization algorithm\n",
        "}\n",
        "\n",
        "results_lr = tune_hyperparameters(model_lr, param_grid_lr, X_train, y_train_encoded, X_test, y_test_encoded)\n",
        "print(f\"Logistic Regression Results: {results_lr}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V_hInxpxRy0"
      },
      "outputs": [],
      "source": [
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
        "    'max_depth': [10, 20, 30],  # Maximum depth of each tree\n",
        "}\n",
        "\n",
        "results_rf = tune_hyperparameters(model_rf, param_grid_rf, X_train, y_train_encoded, X_test, y_test_encoded)\n",
        "print(f\"Random Forest Results: {results_rf}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS6o2rosJfjj"
      },
      "outputs": [],
      "source": [
        "param_grid_knn = {\n",
        "    'n_neighbors': [3, 5, 7, 10],  # Number of neighbors to use\n",
        "    'weights': ['uniform', 'distance'],  # Weight function\n",
        "}\n",
        "\n",
        "results_knn = tune_hyperparameters(model_knn, param_grid_knn, X_train, y_train_encoded, X_test, y_test_encoded)\n",
        "print(f\"KNN Results: {results_knn}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEhJLggDJwvo"
      },
      "outputs": [],
      "source": [
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10, 100],  # Regularization strength\n",
        "    'gamma': [0.01, 0.1, 1, 10],  # Kernel coefficient\n",
        "    'kernel': ['rbf', 'linear'],  # Kernel type (RBF and Linear)\n",
        "}\n",
        "\n",
        "results_svm = tune_hyperparameters(model_svm, param_grid_svm, X_train, y_train_encoded, X_test, y_test_encoded)\n",
        "print(f\"SVM Hyperparameter Tuning Results: {results_svm}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_dt = {\n",
        "    'criterion': ['gini', 'entropy'],  # Split criterion\n",
        "    'max_depth': [5, 10, 15, None],   # Maximum tree depth\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum samples to split\n",
        "    'min_samples_leaf': [1, 2, 4]     # Minimum samples at leaf node\n",
        "}\n",
        "\n",
        "model_dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "results_dt = tune_hyperparameters(model_dt, param_grid_dt, X_train, y_train_encoded, X_test, y_test_encoded)\n",
        "\n",
        "print(f\"Decision Tree Results: {results_dt}\")"
      ],
      "metadata": {
        "id": "VEfw2iNOnHPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Human Activity Recognition"
      ],
      "metadata": {
        "id": "tgNMx5ON7CRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a random sample for testing\n",
        "def random_sample(df):\n",
        "  \"\"\" Retrieve random values for testing prediction\"\"\"\n",
        "\n",
        "  # Randomizse row and select 1st row\n",
        "  random_select= \"\"\"\n",
        "  SELECT *\n",
        "  FROM mobile_health\n",
        "  ORDER BY RANDOM()\n",
        "  LIMIT 10;\n",
        "  \"\"\"\n",
        "\n",
        "  sample_data = pd.read_sql_query(random_select, conn)\n",
        "  # Check the value return\n",
        "  # display(sample_data)\n",
        "  return sample_data"
      ],
      "metadata": {
        "id": "-5niLjta7Rqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def table_test(df, model):\n",
        "  \"\"\" Test Prediction and Display Results \"\"\"\n",
        "  modelname = type(model).__name__\n",
        "\n",
        "  # Get Random Sample\n",
        "  testing = random_sample(df)\n",
        "\n",
        "  # Separate Input variables and results\n",
        "  # # Features\n",
        "  X_features = testing.drop(['Activity', 'subject'], axis=1)\n",
        "  # # Label\n",
        "  y_label = testing['Activity']\n",
        "\n",
        "  # predict\n",
        "  predict = model.predict(X_features)\n",
        "\n",
        "\n",
        "  # Display as table\n",
        "  table = PrettyTable()\n",
        "\n",
        "  # Display Table\n",
        "  \"\"\"\n",
        "  | Test   # |\n",
        "  | Pred Act |\n",
        "  | True Act |\n",
        "  \"\"\"\n",
        "  print(f\"Model : {modelname}\")\n",
        "  table.add_column(\"Test   #\", [\"Pred Act\", \"True Act\"])\n",
        "  for i in range(len(testing)):\n",
        "    table.add_column(str(i+1), [predict[i], y_label[i]])\n",
        "\n",
        "  print(table)"
      ],
      "metadata": {
        "id": "BWQKrMRhMbzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model\n",
        "rf_model = joblib.load('tuned_RF.pkl')\n",
        "\n",
        "# Check model\n",
        "type(rf_model)"
      ],
      "metadata": {
        "id": "XgdQKcyBIvtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_test(df, rf_model)"
      ],
      "metadata": {
        "id": "FCxQuWkhA7AZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "YxfQZgXRpzuM",
        "B4F_NWR8PID1"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}